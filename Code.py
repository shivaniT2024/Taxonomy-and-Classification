# -*- coding: utf-8 -*-
"""MRS_1.py

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1cjuMS9ugsR8ZTTJex2QS7bGMqi1Sq_p8
"""

# define the buckets and pre define the definition and compelxity of the classifications
# goal is genereate embedding in model. Use autoencoder. E -- > encode (loss) --> decode.  after embedding, do k nearest neighbors.

import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.cluster import KMeans
from sklearn.decomposition import PCA
import matplotlib.pyplot as plt
from tensorflow import keras
from tensorflow.keras import layers

df = pd.read_csv('taxonomy_engagement_data (1).csv')

def calculate_engagement_score(row):
    if pd.notnull(row['msEngagement']):
        return row['msEngagement'] + (row['is_like'] * 1) - (row['is_dislike'] * 1)
    return np.nan

df['engagement_score'] = df.apply(calculate_engagement_score, axis=1)

df.fillna(0, inplace=True)

# Select relevant features for encoding
categorical_features = ['media_type', 'artist_style', 'source', 'generated_type']
numerical_features = ['seed', 'num_inference_steps', 'guidance_scale', 'engagement_score']

# One-Hot Encoding
encoder = OneHotEncoder(drop='first', handle_unknown='ignore')
encoded_categorical = encoder.fit_transform(df[categorical_features].astype(str))
encoded_categorical_df = pd.DataFrame(encoded_categorical.toarray(), columns=encoder.get_feature_names_out(categorical_features))
X = pd.concat([df[numerical_features], encoded_categorical_df], axis=1)

# Standardize the numerical features
scaler = StandardScaler()
X[numerical_features] = scaler.fit_transform(X[numerical_features])

# Split the data for autoencoder training
X_train, X_test = train_test_split(X, test_size=0.2, random_state=42)


input_dim = X.shape[1]
autoencoder = keras.Sequential([
    layers.Input(shape=(input_dim,)),
    layers.Dense(128, activation='relu'),
    layers.Dropout(0.2),  # Dropout layer with 20% dropout rate
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(32, activation='relu'),  # Bottleneck
    layers.Dense(64, activation='relu'),
    layers.Dropout(0.2),
    layers.Dense(128, activation='relu'),
    layers.Dense(input_dim, activation='sigmoid')
])

autoencoder.compile(optimizer='adam', loss='mse')

# Train the Autoencoder
history = autoencoder.fit(X_train, X_train, epochs=20, batch_size=256, shuffle=True, validation_data=(X_test, X_test))


X_embeddings = autoencoder.predict(X)

# KMeans Clustering
kmeans = KMeans(n_clusters=11, random_state=42)
kmeans.fit(X_embeddings)

# Assign cluster labels to the original data
df['cluster_labels'] = kmeans.labels_


X_train, X_test, y_train, y_test = train_test_split(X_embeddings, df['cluster_labels'], test_size=0.2, random_state=42)

# Generate PCA for visualization
pca = PCA(n_components=2)
X_train_pca = pca.fit_transform(X_train)

# Visualize clusters
plt.figure(figsize=(10, 7))
plt.scatter(X_train_pca[:, 0], X_train_pca[:, 1], c=y_train, cmap='tab10', s=50, alpha=0.7)
plt.colorbar(label='Cluster')
plt.title('Clusters of KMeans on PCA-Reduced Data')
plt.xlabel('PCA Component 1')
plt.ylabel('PCA Component 2')
plt.show()

from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import classification_report


X_train, X_test, y_train, y_test = train_test_split(X_embeddings, df['cluster_labels'], test_size=0.2, random_state=42)

# Fit KNN model
knn = KNeighborsClassifier(n_neighbors=5)
knn.fit(X_train, y_train)

# Make predictions
y_pred = knn.predict(X_test)

# Evaluate model
print(classification_report(y_test, y_pred, target_names=[f'Cluster {i}' for i in range(11)]))
